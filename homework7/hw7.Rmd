---
title: "STÆ529M Homework 7"
author: "Kári Hlynsson"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE, message = FALSE, warning = TRUE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(rjags)
library(knitr)
library(latex2exp)
library(MASS)
```

# Air quality data

> Fit model $\mathcal M_2$ to the `airquality` data from the previous problem, and use posterior predictive checks to verify that the model fits well. If you find model misspecification, suggest (but do not fit) alternatives. The models are $$
> \begin{align*}
> &\mathcal M_1 : \mathrm{(ozone)}_i \sim \mathrm{Normal}\big(\beta_1 + \beta_2 (\mathrm{solar.R})_i, \sigma^2\big) \\
> &\mathcal M_2: \mathrm{(ozone)}_i \sim \mathrm{Normal}\big(\beta_1 + \beta_2 \mathrm{(solar.R)}_i + \beta_3 \mathrm{(temp)}_i + \beta_4 \mathrm{(wind)}_i, \sigma^2\big).
> \end{align*}
> $$

I changed the coefficients in front of the covariates since there seems to be a typo in the exercise and the authors put $\beta_2$ on both covariates but treat them as separate coefficients in the solution to exercise 1.

Load the data set and and set up the model matrix $\mathbf X_2$:

```{r}
data(airquality)
```

Model matrix and setup:

```{r}
Y <- airquality$Ozone

solar <- scale(airquality$Solar.R)
temp  <- scale(airquality$Temp)
wind  <- scale(airquality$Wind)

X           <- cbind(1, solar, temp, wind)
colnames(X) <- c("(Intercept)", "solar", "temp", "wind")

miss_mask <- is.na(Y + rowSums(X))

X <- X[!miss_mask,]
Y <- Y[!miss_mask]

n <- nrow(X)
p <- ncol(X)

data <- list(X = X, Y = Y, n = n, p = p)
```

We have no information on the $\beta_j$'s and thus it is normal to place an uninformative gaussian prior on them. We will also allow the model to tune the precision $\tau^2 = 1/\sigma^2$ of the likelihood, since it is conjugate and yields a normal posterior. Prepare the JAGS model:

```{r ex1-ppd-lin, fig.cap = "Posterior predictive checks of the $\\mathcal M_2$ model."}
init_airq <- textConnection("model{
  for (i in 1:n) {
    Y[i] ~ dnorm(inprod(X[i,], beta), tau)
  }
  
  for (j in 1:p) {
    beta[j] ~ dnorm(0, 0.0001)
  }
  
  tau ~ dgamma(0.1, 0.1)
  
  for (i in 1:n) {
    Yp[i] ~ dnorm(inprod(X[i,], beta), tau)
  }
  
  D[1] <- min(Yp)
  D[2] <- max(Yp)
  D[3] <- max(Yp) - min(Yp)
  D[4] <- mean(Yp)
  D[5] <- sd(Yp)
}")

model_airq <- jags.model(init_airq, data = data, n.chains = 2,
                         quiet = TRUE)

update(model_airq, n.iter = 1e+4, progress.bar = "none")

samples_airq <- coda.samples(model_airq, variable.names = c("beta", "D", "Yp"),
                             n.iter = 2e+4, progress.bar = "none")

sum_airq <- summary(samples_airq)

D2 <- samples_airq[[1]][,paste0("D[", 1:5, "]")]
D0 <- c(min(Y), max(Y), max(Y) - min(Y), mean(Y), sd(Y))
ss_names <- c("min", "max", "range", "mean", "sd")
pvals <- rep(0, 5); names(pvals) <- ss_names

par(mfrow = c(2,3))
for (i in 1:5) {
  plot(density(D2[,i]), xlim = range(D2[,i], D0[i]),
       main = NA, xlab = TeX(paste0(ss_names[i], "($Y_p$)")))
  abline(v = D0[i], lty = 2)
  
  pvals[i] <- mean(D2[,i] > D0[i])
}
```

The model seems to accurately capture the mean, standard deviation and range of the data. However the same can not be said for the minimum and maximum value. Let's take a look at the $p$-values:

```{r}
pvals
```

The minimum and maximum are near zero, indicating that the model does not fit the scale of the data well. However, the range, mean and standard deviation fit well, indicating that while the model may not be problematic, scaling the data and/or treating outliers might help. Below is a plot that shows the observed values against the fitted ones:

```{r ex1-obs-vs-pred, fig.cap = "Observed versus fitted values in $\\mathcal M_2$."}
Yp <- sum_airq$statistics[paste0("Yp[", 1:111, "]"), 1]

plot(Yp, Y, xlab = TeX("$Y_p$"))
abline(a = 0, b = 1, lty = 2)
```

It seems the model underfits the data at the extremes. This may because of the scale of the reponse, shown in the below histogram (we could also interpret the data as having many outliers, but I think scaling is more appropriate):

```{r ex1-hist-Y, fig.cap = "Histogram of the response, ozone (ppb)."}
hist(Y)
```

Log transforming the response is a potential response to the faults uncovered in the posterior predictive checks. Alternatively, we could use the Box-Cox $\lambda$ transformation where $\lambda$ is the scaling parameter determined by maximum log-likelihood estimation:

```{r ex-1-boxcox, fig.align = "Plot of the log-likelihood estimate of $\\lambda$ in the Box-Cox method."}
par(mfrow = c(1, 2))
boxcox(Y ~ X[,-1])
boxcox(Y ~ X[,-1], lambda = seq(0, 0.5, by = 0.01))
```

From the graph one sees that $\lambda \approx 0.2$. Let's compare all the models so far.

```{r ex1-resid-all, fig.cap = "Residual structures for the normal response model (left), the Box-Cox $\\lambda$ model (middle), and logarithmically-transformed response (right)."}
# normal model
lin <- lm(Y ~ X[,-1])

# linear model with 1/5 transformed response
lin <- lm(Y^(1/5) ~ X[,-1])
pred_lin <- predict(lin, data.frame(X[,-1]))

# linear model with log transformed response
lin_2 <- lm(log(Y) ~ X[,-1])
pred_lin_2<- predict(lin_2, data.frame(X[,-1]))

# residual plots for the models
par(mfrow = c(2, 3), font.main = 1)

plot(lin$resid, ylab = "Residuals", main = "Normal model")
abline(h = 0, lty = 2)

plot(lin$resid, ylab = "Residuals", main = TeX("Box-Cox $\\lambda$ model"))
abline(h = 0, lty = 2)

plot(lin_2$resid, ylab = "Residuals", main = "log(Y) model")
abline(h = 0, lty = 2)

plot(hat(X), ylab = "Leverages", main = "Normal model")
abline(h = 2 * p / n, lty = 2)

plot(hat(X), ylab = "Leverages", main = TeX("Box-Cox $\\lambda$ model"))
abline(h = 2 * p / n, lty = 2)

plot(hat(X), ylab = "Leverages", main = "log(Y) model")
abline(h = 2 * p / n, lty = 2)
```

```{r ex1-pred-all, fig.cap = "Observed response against predicted values in the normal response model (left), Box-Cox $\\lambda$ model (middle), and logarithmically transformed response (right)."}
# predicted vs. fitted
par(mfrow = c(1, 3), font.main = 1)

plot(Y, Yp, xlab = "Y", ylab = "Predicted value", main = "Normal model")
abline(a = 0, b = 1, lty = 2)

plot(Y^(1/5), pred_lin, xlab = TeX("$Y^{1/5}$"), ylab = "Predicted value", main = TeX("Box-Cox $\\lambda$ model"))
abline(a = 0, b = 1, lty = 2)

plot(log(Y), pred_lin_2, xlab = "log(Y)", ylab = "Predicted value", main = TeX("Log transformed model"))
abline(a = 0, b = 1, lty = 2)
```

The logarithmic transformation seemingly diminishes the discrepancy between the fitted and observed data. The largest residual in the $\log(Y)$ model is due to the data point being equal to one, and thus $\log(Y) = 0$ which ends up far from the other observations. The Box-Cox $\lambda$ transformation also performs very well but is nearly identical to the log transformation. Therefore in the sake of simplicity I would suggest log-transforming the response.

Another idea would be to place a $t$-distributed likelihood instead of the normal, since it is more diffuse and therefore might be able to approximate the minimum and maximum better.

# Mr. October

> We use the ''Mr October'' data (Section 2.4) $Y_1 = 563$, $N_1 = 2820$, $Y_2 = 10$ and $N_2 = 27$. Compare the two models $$
> \begin{align*}
> &\mathcal M_1: Y_1 | \lambda_0 \sim \mathrm{Poisson}(N_1\lambda_0) \quad \text{and} \quad        
> Y_2 | \lambda_0 \sim \mathrm{Poisson}(N_2\lambda_0) \\
> &\mathcal M_2: Y_1 | \lambda_1 \sim \mathrm{Poisson}(N_1\lambda_1) \quad \text{and} \quad 
> Y_2 | \lambda_2 \sim \mathrm{Poisson}(N_2\lambda_2)
> \end{align*}
> $$using Bayes factors, DIC and WAIC. Assume the $\text{Uniform}(0, c)$ prior for all $\lambda_j$ and compare the results for $c = 1$ and $c = 10$.

Notice that I have relabelled $\mathcal M_1$ and $\mathcal M_2$ since I feel like the alternative model having a higher index number is more usual. To recap briefly, the data are the total number of games played and number of home runs in regular-season games as well as world series for the American baseball player Reggie Jackson ("Mr. October").

We denote by $Y_1$ and $N_1$ the number of home runs and total number of games in the regular season. World series data are analogously labelled $Y_2$ and $N_2$. In this exercise, we are competing two models $\mathcal{M}_1$ and $\mathcal{M}_2$, where the first model claims that the rate of home runs is different when the player is competing in the World Series or Regular-Season. The latter model claims that there is no difference.

Computing the Bayes factor for the two models requires a bit of preemptive computation. Firstly, note that we assign equal probabilities to $\mathcal M_1$ and $\mathcal M_2$, i.e.

$$
\Pr(\mathcal M_1) = \Pr(\mathcal M_2) = 0.5
$$

such that the Bayes factor becomes

$$
\text{BF}(\mathbf Y) = \frac{\Pr(\mathcal M_2 | \mathbf Y) / \Pr(\mathcal M_2)}{\Pr(\mathcal M_1 | \mathbf Y) /\Pr(\mathcal M_1)} = \frac{\Pr(\mathcal M_2 | \mathbf Y)}{\Pr(\mathcal M_1 | \mathbf Y)}
$$

which resembles an odds ratio and thus the interpretation will be similar. We now set out to derive $\Pr(\mathcal M_2 | \mathbf Y)$ and $\Pr(\mathcal M_1 | \mathbf Y)$. Start with the former:$$
\begin{align*}
\Pr(\mathcal M_1 | \mathbf Y) &= \int_{\Omega_1} \Pr(\mathcal M_1, \mathbf Y) \, d\lambda_0 \\
&= \int_{\Omega_1} \mathcal L(\mathbf Y | \mathcal M_1) \pi(\mathcal M_1) \, d\lambda_0 \\
&= \int_{\Omega_1} \mathcal L(Y_1 | \mathcal M_1) \mathcal L(Y_2 | \mathcal M_1) \pi(\mathcal M_1) \, d\lambda_0\\
&= \frac{1}{c} \int_{\Omega_1} \frac{(N_1\lambda_0)^{Y_1} e^{-N_1\lambda_0}}{\Gamma(Y_1 + 1)} \frac{(N_2\lambda_0)^{Y_2}e^{-N_2\lambda_0}}{\Gamma(Y_2 + 1)} \, d\lambda_0 \\
&= \frac{1}{c} \frac{1}{\Gamma(Y_1 + 1) \Gamma(Y_2 + 1)} \int_{\Omega_1} \lambda_0^{Y_1 + Y_2} e^{-(N_1 + N_2)\lambda_0} \, d\lambda_0 \\ 
&= \frac{1}{c} \frac{\Gamma(Y_1 + Y_2 + 1)}{\Gamma(Y_1 + 1) \Gamma(Y_2 + 1) (N_1 + N_2)^{Y_1 + Y_2 + 1}} \int_{\Omega_1} \frac{(N_1 + N_2)^{Y_1 + Y_2 + 1}}{\Gamma(Y_1 + Y_2 + 1)} \lambda_0^{Y_1 + Y_2 + 1 - 1} e^{-(N_1 + N_2)\lambda_0} d\lambda_0 \\
&= \frac{1}{c} \frac{\Gamma(Y_2 + Y_2 + 1)}{\Gamma(Y_1 + 1) \Gamma(Y_2 + 1) (N_1 + N_2)^{Y_1 + Y_2 + 1}} \Pr(\lambda_0 \leq c)
\end{align*}
$$

where $\Omega_1 = [0,c]$. Continue on to the latter:

$$
\begin{align*}
\Pr(\mathcal M_2 | \mathbf Y) &= \iint_{\Omega_2} \Pr(\mathcal M_2, \mathbf Y) \, d\lambda_1 d\lambda_2 \\
&= \iint_{\Omega_2} \mathcal L(\mathbf Y | \mathcal M_2) \pi(\mathcal M_2) \, d\lambda_1 d\lambda_2 \\
&= \iint_{\Omega_2} \mathcal L(Y_1 | \mathcal M_2) \mathcal L(Y_2 | \mathcal M_2) \pi(\mathcal M_2) \, d\lambda_1 d\lambda_2 \\
&= \frac{1}{c^2}\iint_{\Omega_2} \frac{\lambda_1^{Y_1 + 1 - 1} e^{-N_1\lambda_1}}{\Gamma(Y_1 + 1)} \frac{\lambda_2^{Y_2 + 1 - 1} e^{-N_2\lambda_2}}{\Gamma(Y_2 + 1)} \, d\lambda_1 d\lambda_2 \\
&= \frac{1}{c^2} \frac{1}{N_1^{Y_1 + 1} N_2^{Y_2 + 1}} \Pr(\lambda_1 \leq c) \Pr(\lambda_2 \leq c)
\end{align*}
$$

where $\Omega_2 = [0,c] \times [0,c]$. Now we see that

$$
BF(\mathbf Y) = \frac{\Pr(\mathcal M_2 | \mathbf Y)}{\Pr(\mathcal M_1 |\mathbf Y)} =
\frac{\frac{1}{c^2} \frac{1}{N_1^{Y_1 + 1} N_2^{Y_2 + 1}} \Pr(\lambda_1 \leq c)\Pr(\lambda_2 \leq c)}{\frac{1}{c} \frac{\Gamma(Y_1 + Y_2 + 1)}{\Gamma(Y_1 + 1) \Gamma(Y_2 + 1) (N_1 + N_2)^{Y_1 + Y_2 + 1}} \Pr(\lambda_0 \leq c)}
= \frac{1}{c} \frac{\Gamma(Y_1 + 1)\Gamma(Y_2 + 1)(N_1 + N_2)^{Y_1 + Y_2 + 1}}{\Gamma(Y_1 + Y_2 + 1) N_1^{Y_1 + 1} N_2^{Y_2 + 1}} \frac{\Pr(\lambda_1 \leq c) \Pr(\lambda_2 \leq c)}{\Pr(\lambda_0 \leq c)}
$$

Now we can implement this in R:

```{r}
# Mr. October data
N <- c(2820, 27)
Y <- c(563,  10)

BF <- function(N, Y, c) {
  
  ns_numer <- (Y[1] + Y[2] + 1) * log(N[1] + N[2])
  ns_denom <- (Y[1] + 1) * log(N[1]) + (Y[2] + 1) * log(N[2])
  ns <- exp(ns_numer - ns_denom)
  
  ga_numer <- lgamma(Y[1] + 1) + lgamma(Y[2] + 1)
  ga_denom <- lgamma(Y[1] + Y[2] + 1)
  ga <- exp(ga_numer - ga_denom)
  
  pr_numer <- pgamma(q = c, shape = Y[1] + 1, rate = N[1]) *
              pgamma(q = c, shape = Y[2] + 1, rate = N[2])
  pr_denom <- pgamma(q = c, shape = Y[1] + Y[2] + 1, N[1] + N[2])
  pr <- pr_numer / pr_denom
  
  return(1/c * ns * ga * pr)
}

BF(N, Y, 1)
BF(N, Y, 10)
```

The Bayes' factors are $1.397494$ for $c = 1$ and $0.1397724$ for $c = 10$. In the former case we see that more credibility is assigned to the more advanced model $\mathcal M_2$, whereas in the latter case the simpler model $\mathcal M_1$ is preferred. However neither of these Bayes' factors fall in the decisive range and thus these results are not concrete. We see that the Bayes factor is very sensitive to the prior, since the reciprocal of $c$ enters the expression for the Bayes factor and thus with large values of $c$ the simple model will be preferred.

Now we will move on to the WAIC and DIC. Since we will be repeating the same analyses quite a bit, it is convenient to declare a function which automates this process.

```{r}
# Model parameters
p  <- 2
n  <- 2

# Model strings
str_october_1 <- "model{
  for (i in 1:n) {
    Y[i]     ~ dpois(N[i] * lambda)
    like[i] <- dpois(Y[i], N[i] * lambda)
  }
  
  lambda ~ dunif(0, c)
}"

str_october_2 <- "model{
  for (i in 1:n) {
    Y[i]     ~ dpois(N[i] * lambda[i])
    like[i] <- dpois(Y[i], N[i] * lambda[i])
  }
  
  for (j in 1:p) { lambda[j] ~ dunif(0, c) }
}"

models_october <- c(str_october_1, str_october_2)

# Custom print function for information criterion
print_ic <- function(IC, P) {
  if(class(IC) == "mcarray") { 
    IC <- sum(IC)
    P  <- sum(P)
  }
  cat(paste("- Mean deviance     :", signif(IC, 3), "\n"))
  cat(paste("- Penalty           :", signif(P, 3), "\n"))
  cat(paste("- Penalised deviance:", signif(IC + P, 3), "\n"))
}

# Automates modelling process
mcmc_october <- function(model_no, c) {
  init_october <- textConnection(models_october[model_no])

  data <- list(N = N, Y = Y, c = c, n = n, p = p)
  if (model_no == 1) data <- data[-5]

  model_october <- jags.model(init_october, data = data, 
                              n.chains = 2, quiet = TRUE)

  update(model_october, n.iter = 1e+4, progress.bar = "none")

  DIC     <- dic.samples(model_october, n.iter = 5e+4, progress.bar = "none")
  
  samples <- coda.samples(model_october, variable.names = c("like"),
                          n.iter = 2e+4, progress.bar = "none")
  
  
  like  <- rbind(samples[[1]], samples[[2]])
  L_bar <- colMeans(like)

  Pw <- sum(apply(log(like), 2, var))
  WAIC <- -2 * sum(log(L_bar)) + 2 * Pw
  
  cat(paste0("MODEL: ", model_no, ", C: ", c, "\n"))
  cat("==========================\n")
  cat("WAIC\n")
  print_ic(WAIC, Pw)
  cat("\n")
  cat("DIC\n")
  print_ic(DIC$deviance, DIC$penalty)
  cat("==========================\n\n")
}
```

Running the above function for the models $\mathcal{M}_1$ and $\mathcal{M}_2$ with $c = 1$ yields the result shown below:

```{r}
mcmc_october(model_no = 1, c = 1)
mcmc_october(model_no = 2, c = 1)
mcmc_october(model_no = 1, c = 10)
mcmc_october(model_no = 2, c = 10)
```

Judging from the above output $\mathcal{M}_2$ seems to be the favoured model, and both the $\textit{DIC}$ and $\textit{WAIC}$ agree.

# WWW usage

> Download the `WWWusage` dataset in `R`. Using the data from times $t = 5, \ldots, 100$ as outcomes (earlier times may be used as covariates), fit the autoregressive model $$
> \mathcal M_\ell: Y_t | Y_{t - 1}, \ldots, Y_1 \sim \text{Normal}(\beta_0 + \beta_1 Y_{t - 1} + \cdots \beta_\ell Y_{t - \ell}, \sigma^2)
> $$ where $Y_t$ is the WWW usage at time $t$. Compare the models $\mathcal M_\ell$ with $\ell = 1, 2, 3, 4$ and select the best time lag $\ell$.

```{r}
data(WWWusage)
plot(WWWusage)
```

Pretty. Define a function to fit an autoregressive model $\mathcal M_\ell$ where the lag time $\ell$ enters as an input parameter:

```{r cache = TRUE}
Y <- WWWusage
n <- length(Y)

AR <- function(l) {
  data <- list(Y = Y, l = l, n = n)
  
  autoreg_init <- textConnection("model{
    for (t in 5:n) {
      Y[t]     ~ dnorm(alpha + inprod(beta, Y[t-1:l]), tau)
      like[t] <- dnorm(Y[t], alpha + inprod(beta, Y[t-1:l]), tau)
    }
    
    alpha ~ dnorm(0, 0.001)
    tau   ~ dgamma(0.1, 0.1)
    for (j in 1:l) { beta[j] ~ dnorm(0, 0.001) }
    
  }")
  
  model_autoreg <- jags.model(autoreg_init, data = data, n.chains = 2, quiet = TRUE)
  update(model_autoreg, 1e+4, progress.bar = "none")
  
  samples <- coda.samples(model_autoreg, variable.names = c("like"), n.iter = 2e+4,
                          progress.bar = "none")

  DIC <- dic.samples(model_autoreg, n.iter = 5e+4, progress.bar = "none")
  
  like  <- rbind(samples[[1]], samples[[2]])
  L_bar <- colMeans(like)
  Pw    <- sum(apply(log(like), 2, var))
  WAIC <- -2 * sum(log(L_bar)) + 2 * Pw
  
  return(c(WAIC, Pw, sum(DIC$deviance) + sum(DIC$penalty), sum(DIC$penalty)))
}

AR1 <- AR(1)
AR2 <- AR(2)
AR3 <- AR(3)
AR4 <- AR(4)

AR_dataframe <- rbind(AR1, AR2, AR3, AR4) |> as.data.frame()
rownames(AR_dataframe) <- c("$\\ell = 1$", "$\\ell = 2$", "$\\ell = 3$", "$\\ell = 4$")
colnames(AR_dataframe) <- c("$\\text{WAIC}$", "$p_W$", "$\\text{DIC}$", "$p_D$")

kable(AR_dataframe, caption = "WAIC and DIC for the autoregressive model with lag time $\\ell$")
```

The table above suggests that the model with $\ell = 4$ is most appropriate.
